{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import utils\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDNetData(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        self.generate_dataset()\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_data(self):\n",
    "        pass\n",
    "\n",
    "    def generate_dataset(self):\n",
    "        input, m0, lib = self.read_data()\n",
    "        self.L = m0.shape[1]\n",
    "        self.P = m0.shape[0]\n",
    "        self.N = input['nRow'].item() * input['nCol'].item()\n",
    "        self.Nlib = lib[0,0].shape[1]\n",
    "        self.nr, self.nc = input['nRow'].item(), input['nCol'].item()\n",
    "        self.Y = torch.from_numpy(input['X']).type(torch.float32).reshape(self.N, self.L).T\n",
    "        \n",
    "        SNR = 40 \n",
    "        ssigma = (self.Y.mean())*(10**(-SNR/10))\n",
    "        \n",
    "        self.data_sup = []\n",
    "        for i in range(self.Nlib):\n",
    "            M_s = torch.zeros((self.L,self.P))\n",
    "            for j in range(self.P):\n",
    "                m_ij = lib[0,j][:,i%lib[0,j].shape[1]] # circular shift\n",
    "                M_s[:,j] = torch.from_numpy(m_ij)\n",
    "            for k in range(self.P):\n",
    "                a_s = torch.zeros((self.P,))\n",
    "                a_s[k] = 1.0\n",
    "                y_s = torch.mv(M_s,a_s) + ssigma * torch.randn((self.L,))\n",
    "                self.data_sup.append((y_s,M_s,a_s))\n",
    "                \n",
    "        self.data_unsup = []\n",
    "        for i in range(self.N):\n",
    "            # self.data_unsup.append(torch.from_numpy(self.Y[:,i]).type(torch.float32))\n",
    "            self.data_unsup.append(self.Y[:,i].type(torch.float32))\n",
    "        self.A_gt = -torch.ones((self.P,self.N))\n",
    "        self.A_cube_gt = -torch.ones((self.nr,self.nc,self.P))\n",
    "        self.Mavg_th = torch.ones(self.L)\n",
    "        self.Mn_th = -torch.ones((self.L,self.P,self.N))\n",
    "\n",
    "    def getdata(self):\n",
    "        return self.Y, self.data_sup, self.data_unsup\n",
    "    \n",
    "    def get_gt(self):\n",
    "        return self.A_gt, self.A_cube_gt, self.Mavg_th, self.Mn_th\n",
    "    \n",
    "class JasperRidgeData(IDNetData):\n",
    "    '''\n",
    "        Jasper Ridge Data\n",
    "\n",
    "        Params:\n",
    "            vca: bool\n",
    "                Indicate the reference used for generate the endmember library.\n",
    "                If it is true, VCA is used. In other case, NFINDR.\n",
    "    '''\n",
    "    def __init__(self, vca=True) -> None:\n",
    "        if vca:\n",
    "            self.eea, self.lib_type = 'VCA', 'Lib_vca'\n",
    "        else:\n",
    "            self.eea, self.lib_type = 'NFINDR', 'Lib_nfindr'\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "    def read_data(self):\n",
    "        input = io.loadmat(os.path.join(config.DATASET_PATH, 'jasperRidge/matlab/input.mat'))\n",
    "        m0 = io.loadmat(os.path.join(config.DATASET_PATH, 'jasperRidge/matlab/endmember_estimation.mat'))[self.eea]\n",
    "        lib = io.loadmat(os.path.join(config.DATASET_PATH, 'jasperRidge/matlab/extracted_bundles.mat'))[self.lib_type]\n",
    "        return input, m0, lib\n",
    "    \n",
    "class SamsonData(IDNetData):\n",
    "    '''\n",
    "        Samson Data\n",
    "\n",
    "        Params:\n",
    "            vca: bool\n",
    "                Indicate the reference used for generate the endmember library.\n",
    "                If it is true, VCA is used. In other case, NFINDR.\n",
    "    '''\n",
    "    def __init__(self, vca=True) -> None:\n",
    "        if vca:\n",
    "            self.eea, self.lib_type = 'VCA', 'Lib_vca'\n",
    "        else:\n",
    "            self.eea, self.lib_type = 'NFINDR', 'Lib_nfindr'\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "    def read_data(self):\n",
    "        input = io.loadmat(os.path.join(config.DATASET_PATH, 'samson/matlab/input.mat'))\n",
    "        m0 = io.loadmat(os.path.join(config.DATASET_PATH, 'samson/matlab/endmember_estimation.mat'))[self.eea]\n",
    "        lib = io.loadmat(os.path.join(config.DATASET_PATH, 'samson/matlab/extracted_bundles.mat'))[self.lib_type]\n",
    "        return input, m0, lib\n",
    "    \n",
    "class ApexData(IDNetData):\n",
    "    '''\n",
    "        Apex Data\n",
    "\n",
    "        Params:\n",
    "            vca: bool\n",
    "                Indicate the reference used for generate the endmember library.\n",
    "                If it is true, VCA is used. In other case, NFINDR.\n",
    "    '''\n",
    "    def __init__(self, vca=True) -> None:\n",
    "        if vca:\n",
    "            self.eea, self.lib_type = 'VCA', 'Lib_vca'\n",
    "        else:\n",
    "            self.eea, self.lib_type = 'NFINDR', 'Lib_nfindr'\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "    def read_data(self):\n",
    "        input = io.loadmat(os.path.join(config.DATASET_PATH, 'apex/matlab/input.mat'))\n",
    "        m0 = io.loadmat(os.path.join(config.DATASET_PATH, 'apex/matlab/endmember_estimation.mat'))[self.eea]\n",
    "        lib = io.loadmat(os.path.join(config.DATASET_PATH, 'apex/matlab/extracted_bundles.mat'))[self.lib_type]\n",
    "        return input, m0, lib\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_maker(torch.utils.data.Dataset):\n",
    "    ''' \n",
    "        Pytorch Dataset for IDNet.\n",
    "\n",
    "        Params:\n",
    "            data_opt: int\n",
    "                Indicate the dataset to load.\n",
    "            vca: bool\n",
    "                Indicate the reference used for generate the endmember library.\n",
    "                If it is true, VCA is used. In other case, NFINDR.\n",
    "    '''\n",
    "    def __init__(self, data: IDNetData):\n",
    "        \n",
    "\n",
    "        '''initialize variables and select which data to load\n",
    "        data_opt = 1 : synthetic nonlinear mixture (DC1, with the BLMM) \n",
    "        data_opt = 2 : synthetic example with variability (DC2)\n",
    "        data_opt = 3--5 : real data examples (samson, jasper, cuprite)\n",
    "        '''\n",
    "\n",
    "        self.Y, self.data_sup, self.data_unsup = data.getdata()\n",
    "        self.A_u, self.A_u_cube, self.M_u_avg, self.M_u_ppx = data.get_gt()        \n",
    "        self.L, self.P, self.N = data.L, data.P, data.N\n",
    "\n",
    "        if len(self.data_sup) < len(self.data_unsup):\n",
    "            self.flag_unsup_is_bigger = True\n",
    "        else:\n",
    "            self.flag_unsup_is_bigger = False       \n",
    "        \n",
    "    def __len__(self):\n",
    "        # take the maximum length between the supervised and unsupervised datasets\n",
    "        return max(len(self.data_sup),len(self.data_unsup))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # now, idx corresponds to the index among the largest (sup or unsup) dataset.\n",
    "        # We can multiply it by the ratio between the smallest and the largest datset\n",
    "        # and round down to an integer, to obtain the corresponding index for the \n",
    "        # smaller dataset\n",
    "        if self.flag_unsup_is_bigger:\n",
    "            idx_sup = int(np.floor(idx*len(self.data_sup)/len(self.data_unsup)))\n",
    "            idx_unsup = idx\n",
    "        else:\n",
    "            idx_sup = idx\n",
    "            idx_unsup = int(np.floor(idx*len(self.data_unsup)/len(self.data_sup)))\n",
    "        \n",
    "        # return tuples? (y) and (y,M,a)\n",
    "        return self.data_unsup[idx_unsup], self.data_sup[idx_sup]\n",
    "    \n",
    "    \n",
    "    def plot_training_EMs(self, EM_idx=-1):\n",
    "        '''small method to plot EMs in the training dataset'''\n",
    "        L, P, Nsamp = self.data_sup[0][1].shape[0], self.data_sup[0][1].shape[1], len(self.data_sup)\n",
    "        M_train = torch.zeros((L,P,Nsamp))\n",
    "        for i in range(Nsamp):\n",
    "            M_train[:,:,i] = self.data_sup[0][1]\n",
    "        if EM_idx == -1:\n",
    "            fig, axs = plt.subplots(1, P)\n",
    "            for i in range(P):\n",
    "                axs[i].plot(torch.squeeze(M_train[:,i,:]))\n",
    "        else:\n",
    "            plt.figure()\n",
    "            plt.plot(torch.squeeze(M_train[:,EM_idx,:]))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_IDNet import IDNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, log_probs_unsup, log_probs_sup, omegas_nrmlzd, log_omegas, alphas_all, \n",
    "                     llambda, bbeta, tau, lamb_We, lamb_Wd):\n",
    "    # llambda = my_llambda; # regularization between supervised and unsupervised part\n",
    "    # bbeta   = 10; # extra regularization (high likelihood of endmembers and abundances training data in the posterior)\n",
    "    # tau     = my_tau; # extra extra regularization (sparsity)\n",
    "    # lamb_We = my_lamb_We; # penalizes network weights of nonlinear encoder\n",
    "    # lamb_Wd = my_lamb_Wd; # penalizes network weights of nonlinear decoder\n",
    "    \n",
    "    K1 = log_probs_unsup['log_py_Ma'].shape[1]\n",
    "    K2 = omegas_nrmlzd.shape[1]\n",
    "    batch_size = omegas_nrmlzd.shape[0]\n",
    "    \n",
    "    # unsupervised part of the cost function --------------\n",
    "    cost_unsup = 0\n",
    "    for i in range(0,batch_size):\n",
    "        for j in range(0,K1):\n",
    "            # terms in the numerator\n",
    "            cost_unsup = cost_unsup + log_probs_unsup['log_py_Ma'][i,j] \n",
    "            cost_unsup = cost_unsup + log_probs_unsup['log_pa'][i,j]\n",
    "            cost_unsup = cost_unsup + log_probs_unsup['log_pM_Z'][i,j] \n",
    "            cost_unsup = cost_unsup + log_probs_unsup['log_pZ'][i,j]\n",
    "            # terms in the denominator\n",
    "            cost_unsup = cost_unsup - log_probs_unsup['log_qa_My'][i,j]\n",
    "            cost_unsup = cost_unsup - log_probs_unsup['log_qM_Z'][i,j]\n",
    "            cost_unsup = cost_unsup - log_probs_unsup['log_qZ_y'][i,j]\n",
    "            \n",
    "    # supervised part of the cost function  \n",
    "    cost_sup = 0\n",
    "    for i in range(0,batch_size):\n",
    "        for j in range(0,K2):\n",
    "            temp = 0\n",
    "            # terms in the numerator\n",
    "            temp = temp + log_probs_sup['log_py_Ma'][i,j] \n",
    "            temp = temp + log_probs_sup['log_pa'][i,j] \n",
    "            temp = temp + log_probs_sup['log_pM_Z'][i,j] \n",
    "            temp = temp + log_probs_sup['log_pZ'][i,j] \n",
    "            # terms in the denominator\n",
    "            temp = temp - log_probs_sup['log_qa_My'][i,j] \n",
    "            temp = temp - log_probs_sup['log_qM_Z'][i,j] \n",
    "            temp = temp - log_probs_sup['log_qZ_y'][i,j] \n",
    "            # importance weight normalization (the omegas are already normalized now)\n",
    "            temp = omegas_nrmlzd[i,j] * temp\n",
    "            # accumulate in the cost function\n",
    "            cost_sup = cost_sup + temp\n",
    "            \n",
    "    # regularization term\n",
    "    cost_reg = 0\n",
    "    for i in range(0,batch_size):\n",
    "        for j in range(0,K2):\n",
    "            cost_reg = cost_reg + log_probs_sup['log_qa_My'][i,j] \n",
    "            cost_reg = cost_reg + log_omegas[i,j]\n",
    "    \n",
    "    # yet another regularization term (sparsity on alphas)\n",
    "    cost_reg_sprs = 0\n",
    "    for i in range(0,batch_size):\n",
    "        # this one computes it over the unsupervised data\n",
    "        for j in range(0,K1):\n",
    "            cost_reg_sprs = cost_reg_sprs + torch.linalg.norm(alphas_all['alphas_unsup'][:,i,j], ord=0.5) / K1\n",
    "        # this one computes it over the supervised data\n",
    "        for j in range(0,K2):\n",
    "            cost_reg_sprs = cost_reg_sprs + torch.linalg.norm(alphas_all['alphas_sup'][:,i,j], ord=0.5) / K2\n",
    "    \n",
    "    \n",
    "    # regularizes nonlinear mixing weights\n",
    "    reg_nlin_d_weights = 0\n",
    "    for param in model.fcy_Ma_nlin.parameters():\n",
    "        reg_nlin_d_weights = reg_nlin_d_weights + torch.norm(param, p=\"fro\")\n",
    "    \n",
    "    reg_nlin_e_weights = 0\n",
    "    for param in model.fca_My_alphas.parameters():\n",
    "        reg_nlin_e_weights = reg_nlin_e_weights + torch.norm(param, p=\"fro\")    \n",
    "        \n",
    "    \n",
    "    # now the total cost functions\n",
    "    cost = cost_unsup/K1 + llambda * cost_sup/K2 + (1+bbeta) * cost_reg/K2 \\\n",
    "           - tau * cost_reg_sprs - lamb_We * reg_nlin_e_weights - lamb_Wd * reg_nlin_d_weights\n",
    "    return -cost # maximize cost\n",
    "\n",
    "\n",
    "def train(epoch, model, optimizer, train_loader, llambda, bbeta, tau, lamb_We, lamb_Wd):\n",
    "    log_interval = 100 # how many batches to wait before logging training status    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # get one batch from supervised data and from unsupervised data\n",
    "    for batch_idx, alldata in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        log_probs_unsup, log_probs_sup, omegas_nrmlzd, log_omegas, alphas_all = model(alldata)\n",
    "        loss = loss_function(model, log_probs_unsup, log_probs_sup, omegas_nrmlzd, log_omegas, alphas_all,\n",
    "                             llambda, bbeta, tau, lamb_We, lamb_Wd)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(alldata), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(alldata)))\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, avg_loss))\n",
    "    return avg_loss\n",
    "\n",
    "def test(epoch, model, train_loader):\n",
    "    model.eval()\n",
    "    L, N = train_loader.dataset.L, train_loader.dataset.N\n",
    "    with torch.no_grad():\n",
    "        Y = torch.zeros((L,N))\n",
    "        for i in range(N):\n",
    "            Y[:,i] = train_loader.dataset.data_unsup[i]\n",
    "        A_est, Mn_est, M_avg, Y_rec, a_nlin_deg = model.unmix(Y)\n",
    "        \n",
    "    return A_est, Mn_est, Y_rec, a_nlin_deg, M_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\") # The implementation does not work on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration proposed by the Authors\n",
    "my_llambda = 1      # regularization between supervised and unsupervised part\n",
    "my_bbeta = 10       # extra regularization (high likelihood of endmembers and abundances training data in the posterior)\n",
    "my_tau     = 0.005  # extra extra regularization (sparsity)\n",
    "my_lamb_We = 0.01   # penalizes network weights of nonlinear encoder\n",
    "my_lamb_Wd = 0.1    # penalizes network weights of nonlinear decoder\n",
    "\n",
    "    # llambda = my_llambda; \n",
    "    # bbeta   = 10; # extra regularization (high likelihood of endmembers and abundances training data in the posterior)\n",
    "    # tau     = my_tau; # extra extra regularization (sparsity)\n",
    "    # lamb_We = my_lamb_We; # penalizes network weights of nonlinear encoder\n",
    "    # lamb_Wd = my_lamb_Wd; # penalizes network weights of nonlinear decoder\n",
    "\n",
    "vca = True\n",
    "dataset_name = 'Samson_'+('VCA' if vca else 'NFINDR')\n",
    "\n",
    "dataset = dataset_maker(SamsonData(vca=vca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "L = train_loader.dataset.data_sup[0][1].shape[0]\n",
    "P = train_loader.dataset.data_sup[0][1].shape[1]\n",
    "N = len(train_loader.dataset.data_unsup)\n",
    "nr, nc = train_loader.dataset.A_u_cube.shape[0], train_loader.dataset.A_u_cube.shape[1]\n",
    "H = 2 # dimension of the latent EM space\n",
    "\n",
    "\n",
    "model = IDNet(P, L, H=H).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "start_time = time.time()\n",
    "num_epochs = 30; # number of epochs to train\n",
    "loss_old = 1e30\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss_t = train(epoch, model, optimizer, train_loader, my_llambda, my_bbeta, my_tau, my_lamb_We, my_lamb_Wd)\n",
    "    A_est, Mn_est, Y_rec, a_nlin_deg, M_avg = test(epoch, model, train_loader)\n",
    "    \n",
    "    # compute metrics -----------------------\n",
    "    RMSE_A, NRMSE_A = utils.compute_metrics(train_loader.dataset.A_u, A_est)\n",
    "    RMSE_M, NRMSE_M = utils.compute_metrics(train_loader.dataset.M_u_ppx, Mn_est)\n",
    "    RMSE_Y, NRMSE_Y = utils.compute_metrics(train_loader.dataset.Y, Y_rec)\n",
    "    \n",
    "    metrics_str = '====> EPOCH: {:d}, Abundance NRMSE: {:.6f}, Endmember NRMSE: {:.6f}'.format(epoch, NRMSE_A, NRMSE_M)\n",
    "    with open(os.path.join('results', f'{dataset_name}.txt'), \"a\") as text_file:\n",
    "        print(metrics_str, file=text_file)\n",
    "        print(metrics_str) # print to console too\n",
    "    \n",
    "    if epoch <= 10:\n",
    "        scheduler.step() # reduce from 1e-3 to 1e-4 in 10 epochs with rate approx 0.8\n",
    "    \n",
    "    # check stopping condition\n",
    "    if abs(loss_t - loss_old) / abs(loss_t) < 1e-2:\n",
    "        break\n",
    "    loss_old = loss_t\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "\n",
    "# plot abundances and average EMs ----------------------------------\n",
    "utils.plotAbunds(A_est, nr=nr, nc=nc, \n",
    "                    thetitle='learned abundances',\n",
    "                    savepath=os.path.join('results', f'{dataset_name}.pdf'))\n",
    "utils.plotEMs(M_avg, thetitle='learned avg EMs')\n",
    "\n",
    "# compare results to ground truth abundances if available\n",
    "# utils.show_ground_truth(A_true=train_loader.dataset.A_u, Mgt_avg=train_loader.dataset.M_u_avg, nr=nr, nc=nc)\n",
    "\n",
    "\n",
    "print('====> FINAL: Abundance NRMSE: {:.6f}, Endmember NRMSE: {:.6f}'.format(NRMSE_A, NRMSE_M))\n",
    "print('====> Elapsed time: {:.6f}'.format(elapsed_time))\n",
    "# plt.figure(), plt.plot(Mn_est[:,0,0:20]), plt.show();\n",
    "\n",
    "# save .mat file with the results\n",
    "io.savemat(os.path.join('results', f'{dataset_name}.mat'), \n",
    "        {'A_est':A_est.numpy(),\n",
    "            'Mn_est':Mn_est.numpy(),\n",
    "            'Y_rec':Y_rec.numpy(),\n",
    "            'a_nlin_deg':a_nlin_deg.numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('====> FINAL: Abundance NRMSE: {:.6f}, Endmember NRMSE: {:.6f}'.format(NRMSE_A, NRMSE_M))\n",
    "print('====> Elapsed time: {:.6f}'.format(elapsed_time))\n",
    "# plt.figure(), plt.plot(Mn_est[:,0,0:20]), plt.show();\n",
    "\n",
    "# save .mat file with the results\n",
    "io.savemat(os.path.join('results', f'{dataset_name}.mat'), \n",
    "        {'A_est':A_est.numpy(),\n",
    "            'Mn_est':Mn_est.numpy(),\n",
    "            'Y_rec':Y_rec.numpy(),\n",
    "            'a_nlin_deg':a_nlin_deg.numpy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jasper Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_llambda = 1      # regularization between supervised and unsupervised part\n",
    "my_bbeta = 10       # extra regularization (high likelihood of endmembers and abundances training data in the posterior)\n",
    "my_tau     = 0      # extra extra regularization (sparsity)\n",
    "my_lamb_We = 0.01   # penalizes network weights of nonlinear encoder\n",
    "my_lamb_Wd = 0.1    # penalizes network weights of nonlinear decoder\n",
    "\n",
    "vca = True\n",
    "dataset_name = 'JasperRidge_'+('VCA' if vca else 'NFINDR')\n",
    "\n",
    "dataset = dataset_maker(JasperRidgeData(vca=vca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "L = train_loader.dataset.data_sup[0][1].shape[0]\n",
    "P = train_loader.dataset.data_sup[0][1].shape[1]\n",
    "N = len(train_loader.dataset.data_unsup)\n",
    "nr, nc = train_loader.dataset.A_u_cube.shape[0], train_loader.dataset.A_u_cube.shape[1]\n",
    "H = 2 # dimension of the latent EM space\n",
    "\n",
    "\n",
    "model = IDNet(P, L, H=H).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "start_time = time.time()\n",
    "num_epochs = 30; # number of epochs to train\n",
    "loss_old = 1e30\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss_t = train(epoch, model, optimizer, train_loader, my_llambda, my_bbeta, my_tau, my_lamb_We, my_lamb_Wd)\n",
    "    A_est, Mn_est, Y_rec, a_nlin_deg, M_avg = test(epoch, model, train_loader)\n",
    "    \n",
    "    # compute metrics -----------------------\n",
    "    RMSE_A, NRMSE_A = utils.compute_metrics(train_loader.dataset.A_u, A_est)\n",
    "    RMSE_M, NRMSE_M = utils.compute_metrics(train_loader.dataset.M_u_ppx, Mn_est)\n",
    "    RMSE_Y, NRMSE_Y = utils.compute_metrics(train_loader.dataset.Y, Y_rec)\n",
    "    \n",
    "    metrics_str = '====> EPOCH: {:d}, Abundance NRMSE: {:.6f}, Endmember NRMSE: {:.6f}'.format(epoch, NRMSE_A, NRMSE_M)\n",
    "    with open(os.path.join('results', f'{dataset_name}.txt'), \"a\") as text_file:\n",
    "        print(metrics_str, file=text_file)\n",
    "        print(metrics_str) # print to console too\n",
    "    \n",
    "    if epoch <= 10:\n",
    "        scheduler.step() # reduce from 1e-3 to 1e-4 in 10 epochs with rate approx 0.8\n",
    "    \n",
    "    # check stopping condition\n",
    "    if abs(loss_t - loss_old) / abs(loss_t) < 1e-2:\n",
    "        break\n",
    "    loss_old = loss_t\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "\n",
    "# plot abundances and average EMs ----------------------------------\n",
    "utils.plotAbunds(A_est, nr=nr, nc=nc, \n",
    "                    thetitle='learned abundances',\n",
    "                    savepath=os.path.join('results', f'{dataset_name}.pdf'))\n",
    "utils.plotEMs(M_avg, thetitle='learned avg EMs')\n",
    "\n",
    "# compare results to ground truth abundances if available\n",
    "# utils.show_ground_truth(A_true=train_loader.dataset.A_u, Mgt_avg=train_loader.dataset.M_u_avg, nr=nr, nc=nc)\n",
    "\n",
    "\n",
    "print('====> FINAL: Abundance NRMSE: {:.6f}, Endmember NRMSE: {:.6f}'.format(NRMSE_A, NRMSE_M))\n",
    "print('====> Elapsed time: {:.6f}'.format(elapsed_time))\n",
    "# plt.figure(), plt.plot(Mn_est[:,0,0:20]), plt.show();\n",
    "\n",
    "# save .mat file with the results\n",
    "io.savemat(os.path.join('results', f'{dataset_name}.mat'), \n",
    "        {'A_est':A_est.numpy(),\n",
    "            'Mn_est':Mn_est.numpy(),\n",
    "            'Y_rec':Y_rec.numpy(),\n",
    "            'a_nlin_deg':a_nlin_deg.numpy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_llambda = 1      # regularization between supervised and unsupervised part\n",
    "my_bbeta = 10       # extra regularization (high likelihood of endmembers and abundances training data in the posterior)\n",
    "my_tau     = 0.01      # extra extra regularization (sparsity)\n",
    "my_lamb_We = 0.005   # penalizes network weights of nonlinear encoder\n",
    "my_lamb_Wd = 0.1    # penalizes network weights of nonlinear decoder\n",
    "\n",
    "vca = True\n",
    "dataset_name = 'Apex'+('VCA' if vca else 'NFINDR')\n",
    "\n",
    "dataset = dataset_maker(ApexData(vca=vca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "L = train_loader.dataset.data_sup[0][1].shape[0]\n",
    "P = train_loader.dataset.data_sup[0][1].shape[1]\n",
    "N = len(train_loader.dataset.data_unsup)\n",
    "nr, nc = train_loader.dataset.A_u_cube.shape[0], train_loader.dataset.A_u_cube.shape[1]\n",
    "H = 2 # dimension of the latent EM space\n",
    "\n",
    "\n",
    "model = IDNet(P, L, H=H).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "start_time = time.time()\n",
    "num_epochs = 30; # number of epochs to train\n",
    "loss_old = 1e30\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss_t = train(epoch, model, optimizer, train_loader, my_llambda, my_bbeta, my_tau, my_lamb_We, my_lamb_Wd)\n",
    "    A_est, Mn_est, Y_rec, a_nlin_deg, M_avg = test(epoch, model, train_loader)\n",
    "    \n",
    "    # compute metrics -----------------------\n",
    "    RMSE_A, NRMSE_A = utils.compute_metrics(train_loader.dataset.A_u, A_est)\n",
    "    RMSE_M, NRMSE_M = utils.compute_metrics(train_loader.dataset.M_u_ppx, Mn_est)\n",
    "    RMSE_Y, NRMSE_Y = utils.compute_metrics(train_loader.dataset.Y, Y_rec)\n",
    "    \n",
    "    metrics_str = '====> EPOCH: {:d}, Abundance NRMSE: {:.6f}, Endmember NRMSE: {:.6f}'.format(epoch, NRMSE_A, NRMSE_M)\n",
    "    with open(os.path.join('results', f'{dataset_name}.txt'), \"a\") as text_file:\n",
    "        print(metrics_str, file=text_file)\n",
    "        print(metrics_str) # print to console too\n",
    "    \n",
    "    if epoch <= 10:\n",
    "        scheduler.step() # reduce from 1e-3 to 1e-4 in 10 epochs with rate approx 0.8\n",
    "    \n",
    "    # check stopping condition\n",
    "    if abs(loss_t - loss_old) / abs(loss_t) < 1e-2:\n",
    "        break\n",
    "    loss_old = loss_t\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "\n",
    "# plot abundances and average EMs ----------------------------------\n",
    "utils.plotAbunds(A_est, nr=nr, nc=nc, \n",
    "                    thetitle='learned abundances',\n",
    "                    savepath=os.path.join('results', f'{dataset_name}.pdf'))\n",
    "utils.plotEMs(M_avg, thetitle='learned avg EMs')\n",
    "\n",
    "# compare results to ground truth abundances if available\n",
    "# utils.show_ground_truth(A_true=train_loader.dataset.A_u, Mgt_avg=train_loader.dataset.M_u_avg, nr=nr, nc=nc)\n",
    "\n",
    "\n",
    "print('====> FINAL: Abundance NRMSE: {:.6f}, Endmember NRMSE: {:.6f}'.format(NRMSE_A, NRMSE_M))\n",
    "print('====> Elapsed time: {:.6f}'.format(elapsed_time))\n",
    "# plt.figure(), plt.plot(Mn_est[:,0,0:20]), plt.show();\n",
    "\n",
    "# save .mat file with the results\n",
    "io.savemat(os.path.join('results', f'{dataset_name}.mat'), \n",
    "        {'A_est':A_est.numpy(),\n",
    "            'Mn_est':Mn_est.numpy(),\n",
    "            'Y_rec':Y_rec.numpy(),\n",
    "            'a_nlin_deg':a_nlin_deg.numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
